/*
 * Copyright OpenSearch Contributors
 * SPDX-License-Identifier: Apache-2.0
 */

package com.amazon.dataprepper.plugins.processor.csv;

import com.amazon.dataprepper.metrics.PluginMetrics;
import com.amazon.dataprepper.model.annotations.DataPrepperPlugin;
import com.amazon.dataprepper.model.annotations.DataPrepperPluginConstructor;
import com.amazon.dataprepper.model.event.Event;
import com.amazon.dataprepper.model.processor.AbstractProcessor;
import com.amazon.dataprepper.model.processor.Processor;
import com.amazon.dataprepper.model.record.Record;
import com.fasterxml.jackson.databind.MappingIterator;
import com.fasterxml.jackson.dataformat.csv.CsvMapper;
import com.fasterxml.jackson.dataformat.csv.CsvParser;
import com.fasterxml.jackson.dataformat.csv.CsvSchema;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Collection;
import java.util.List;

/**
 * Processor to parse CSV data in Events.
 *
 */
@DataPrepperPlugin(name="csv", pluginType = Processor.class, pluginConfigurationType = CSVProcessorConfig.class)
public class CSVProcessor extends AbstractProcessor<Record<Event>, Record<Event>> {
    private final List<String> header;
    private final String quote_char;
    private final String delimiter;
    private final String source_key;
    private final String headerSourceKey;
    private final boolean hasUserSpecifiedHeader;
    private final boolean hasHeaderSource;
    private final Boolean isDeleteHeaderAfterProcessing;

    @DataPrepperPluginConstructor
    public CSVProcessor(final PluginMetrics pluginMetrics, final CSVProcessorConfig config) {
        super(pluginMetrics);
        this.source_key = config.getSource();
        this.header = config.getColumnNames();
        this.quote_char = config.getQuoteCharacter();
        this.delimiter = config.getDelimiter();
        this.headerSourceKey = config.getColumnNamesSourceKey();
        this.isDeleteHeaderAfterProcessing = config.isDeleteHeader();

        this.hasUserSpecifiedHeader = (this.header != null);
        this.hasHeaderSource = (this.headerSourceKey != null);
    }

    @Override
    public Collection<Record<Event>> doExecute(final Collection<Record<Event>> records) {
        CsvMapper mapper = createCsvMapper();
        CsvSchema schema = createCsvSchema();

        for (Record<Event> record : records) {

            Event event = record.getData();

            String message = event.get(this.source_key, String.class);
            boolean this_event_has_header_source = this.hasHeaderSource && event.containsKey(this.headerSourceKey);

            try {
                MappingIterator<List<String>> message_it = mapper.readerFor(List.class).with(schema).readValues(message);

                // otherwise the message is empty
                if (message_it.hasNextValue()) {
                    List<String> row = message_it.nextValue();

                    if (this_event_has_header_source) {
                        String header_unprocessed = event.get(this.headerSourceKey, String.class);
                        MappingIterator<List<String>> header_it = mapper.readerFor(List.class).with(schema).readValues(header_unprocessed);
                        // if header is empty, behaves correctly since columns are autogenerated.
                        List<String> header_from_event_source = header_it.nextValue();
                        putDataInEvent(event, header_from_event_source, row);
                    } else {
                        if (this.hasUserSpecifiedHeader) {
                            putDataInEvent(event, this.header, row);
                        }
                        else {
                            List<String> emptyHeader = new ArrayList<String>();
                            putDataInEvent(event, emptyHeader, row);
                        }
                    }
                }

                if (this_event_has_header_source && this.isDeleteHeaderAfterProcessing) {
                    event.delete(this.headerSourceKey);
                }
            } catch (IOException e) {
                System.out.println(e.toString());
            }
        }
        return records;
    }

    @Override
    public void prepareForShutdown() {

    }

    @Override
    public boolean isReadyForShutdown() {
        return true;
    }

    @Override
    public void shutdown() {

    }

    private CsvMapper createCsvMapper() {
        CsvMapper mapper = new CsvMapper();
        mapper.enable(CsvParser.Feature.WRAP_AS_ARRAY); // allows mapper to read with empty schema
        return mapper;
    }

    private CsvSchema createCsvSchema() {
        char delimiter_as_char = this.delimiter.charAt(0); // safe due to config input validations
        char quote_char_as_char = this.quote_char.charAt(0); // safe due to config input validations
        CsvSchema schema = CsvSchema.emptySchema().withColumnSeparator(delimiter_as_char).withQuoteChar(quote_char_as_char);
        return schema;
    }

    private void putDataInEvent(Event event, List<String> header, List<String> data) {
        // open question: We decided that if there is a user-specified header and a header source event key then we use the header source
        // key as the source of truth. What if the user-specified header has the right number of rows, but the header key has too few rows?
        // Right now the extra columns are autogenerated, but we could use the user-specified header. Keeping the current implementation
        // might be less confusing, but also has less functionality/control over the header values.
        int dataSize = data.size();
        int headerSize = header.size();
        if (dataSize <= headerSize) {
            for (int i = 0; i < dataSize; i++) {
                event.put(header.get(i),data.get(i));
            }
        } else {
            int i = 0;
            for (; i < headerSize; i++) {
                event.put(header.get(i),data.get(i));
            }
            for (int j = i; j < dataSize; j++) {
                event.put(generateColumn(j),data.get(j));
            }
        }
    }

    private String generateColumn(int col_number) {
        int display_col_number = col_number + 1; // just like logstash we 1-index columns
        return "column" + display_col_number;
    }
}
